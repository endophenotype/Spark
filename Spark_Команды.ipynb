{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTVa4C9Il2PeNvotmQgaj6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/endophenotype/Spark/blob/main/Spark_%D0%9A%D0%BE%D0%BC%D0%B0%D0%BD%D0%B4%D1%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMXcmvz9czSG"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "ZBakr-9ue0re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fofQWtDafK9C",
        "outputId": "7bd9a0f9-a720-42f4-de63-418327d84774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "9nSxSa8sfPnA",
        "outputId": "19d28fb4-bcb2-4397-ccd6-958a718e962b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f37a01fc4f0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://1e29a333cccc:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Параллельная коллекция, содержащая числа от 1\n",
        "до 5:"
      ],
      "metadata": {
        "id": "LBRQVPpPvCnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "appName = 'appName'\n",
        "master = 'local[*]'\n",
        "conf = SparkConf().setAppName(appName).setMaster(master)\n",
        "sc = spark.sparkContext\n",
        "data = [1, 2, 3, 4, 5]\n",
        "distData = sc.parallelize(data)\n",
        "print(distData)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd0KCqqQu5Ub",
        "outputId": "d2f2fad3-9147-4d7e-8dd1-1040be58be22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "После создания распределенный набор данных ( distData) может работать\n",
        "параллельно. Например, мы можем вызвать distData.reduce(lambda a, b: a + b),\n",
        "чтобы сложить элементы списка."
      ],
      "metadata": {
        "id": "SRH1FUxFy5xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "appName = 'appName'\n",
        "master = 'local[1]'\n",
        "sc.stop()\n",
        "conf = SparkConf().setAppName(appName).setMaster(master)\n",
        "sc = SparkContext(conf=conf)\n",
        "data = [1, 2, 3, 4, 5]\n",
        "distData = sc.parallelize(data)\n",
        "summ = distData.reduce(lambda a, b: a + b)\n",
        "print(summ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx263i0YxpgA",
        "outputId": "d7c2acdd-8758-4575-9704-259a7b49990f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Текстовые файлы RDD могут быть созданы с использованием\n",
        "SparkContextметода textFile. Этот метод принимает URI для файла (либо\n",
        "локальный путь на машине, либо URI hdfs://, s3a://, и т. д.) и считывает его как\n",
        "набор строк. Вот пример вызова:"
      ],
      "metadata": {
        "id": "Kl1BiuTYy0hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distFile = sc.textFile(\"/content/G.txt\")\n",
        "print(distFile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ninEDbUIx5bQ",
        "outputId": "4ee5a4e4-8a4a-488b-9020-d01b4825a5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/G.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как и текстовые файлы, SequenceFiles можно сохранять и загружать, указав\n",
        "путь."
      ],
      "metadata": {
        "id": "x0OTERQW1FeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()\n",
        "conf = SparkConf().setAppName(appName).setMaster(master)\n",
        "sc = SparkContext(conf=conf)\n",
        "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))\n",
        "sorted(sc.sequenceFile(\"/content/A/part-00000\").collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UPNV85qysw2",
        "outputId": "99425d0c-d9b9-48bf-94e4-0d8139f1a8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'a'), (2, 'aa'), (3, 'aaa')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По умолчанию каждый преобразованный RDD может пересчитываться\n",
        "каждый раз, когда над ним запускается действие. Однако можно\n",
        "сохранить RDD в памяти с помощью метода persist(или cache), и в этом случае\n",
        "Spark сохранит элементы в кластере для более быстрого доступа при\n",
        "следующем запросе. Также поддерживается сохранение RDD на диске или их\n",
        "репликация на нескольких узлах.\n"
      ],
      "metadata": {
        "id": "QQDWurQL1jpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()\n",
        "conf = SparkConf().setAppName(appName).setMaster(master)\n",
        "sc = SparkContext(conf=conf)\n",
        "lines = sc.textFile(\"/content/G.txt\")\n",
        "lineLengths = lines.map(lambda s: len(s))\n",
        "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
        "print(totalLength)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCCy3OWn1Anq",
        "outputId": "99169737-92db-4ce3-ec7c-6544b982148c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лямбда-выражения для простых функций, которые можно записать в виде\n",
        "выражения. (Лямбды не поддерживают функции с несколькими операторами\n",
        "или операторы, которые не возвращают значение.)"
      ],
      "metadata": {
        "id": "pj0npBrs17V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()\n",
        "conf = SparkConf().setAppName(appName).setMaster(master)\n",
        "sc = SparkContext(conf=conf)\n",
        "words = [\"hello this is line one\", \"hello this is line two\"]\n",
        "words_rdd = sc.parallelize(words)\n",
        "print (words_rdd.collect())\n",
        "words_rdd = words_rdd.flatMap(lambda line: line.split(\" \"))\n",
        "print (words_rdd.collect())\n",
        "pairs = words_rdd.map(lambda s: (s, 1))\n",
        "print (pairs.collect())\n",
        "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
        "print (counts.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eoxtqysg1RT4",
        "outputId": "525ac1a4-cf61-43f2-be2d-671f36e058f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello this is line one', 'hello this is line two']\n",
            "['hello', 'this', 'is', 'line', 'one', 'hello', 'this', 'is', 'line', 'two']\n",
            "[('hello', 1), ('this', 1), ('is', 1), ('line', 1), ('one', 1), ('hello', 1), ('this', 1), ('is', 1), ('line', 1), ('two', 1)]\n",
            "[('hello', 2), ('this', 2), ('is', 2), ('line', 2), ('one', 1), ('two', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Локальные defs внутри функции, вызывающей Spark, для более длинного кода.\n",
        "Функции верхнего уровня в модуле.\n",
        "Например, чтобы передать более длинную функцию, чем может\n",
        "поддерживаться с помощью lambda, рассмотрим следующий код:"
      ],
      "metadata": {
        "id": "ZiY6PgOk3Orx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def myFunc(s):\n",
        " word = s.split(\" \")\n",
        " return len(word)\n",
        "words = [\"hello this is line one\", \"hello this is line two\"]\n",
        "count = sc.parallelize(words).map(myFunc)\n",
        "print(count.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDWHJVtf2Blw",
        "outputId": "bcc74396-3279-48b2-d712-eb0df24e333a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим наивную сумму элементов RDD ниже, которая может вести себя\n",
        "по-разному в зависимости от того, происходит ли выполнение в одной и той\n",
        "же JVM. Типичным примером этого является запуск Spark в local режиме ( --\n",
        "master = local[n]) по сравнению с развертыванием приложения Spark в кластере\n",
        "(например, с помощью spark-submit в YARN):\n"
      ],
      "metadata": {
        "id": "kQ_kVndi3W6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [1, 2, 3, 4, 5]\n",
        "accum = sc.accumulator(0)\n",
        "rdd = sc.parallelize(data)\n",
        "def increment_counter(x):\n",
        " accum.add(x)\n",
        "rdd.foreach(increment_counter)\n",
        "print(\"Counter value: \", accum.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMW6QOns3SZl",
        "outputId": "4f737762-7b45-49c2-9bb8-3ec8f4a57c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter value:  15\n"
          ]
        }
      ]
    }
  ]
}
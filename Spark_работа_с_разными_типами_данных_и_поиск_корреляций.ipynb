{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/QMkoBpVqhklbLERcGm7t"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMXcmvz9czSG"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\"\n",
        "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\""
      ],
      "metadata": {
        "id": "aSBIgXsxq3hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "Z6yyIRGmq3hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.pandas as ps\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "spark"
      ],
      "metadata": {
        "id": "Mqo3Tp_CsBC6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "e43a4e5e-30a7-4228-c38d-fa0d26fb0099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe1c5fcc610>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://eb6a45bd19dd:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запись и чтение CSV"
      ],
      "metadata": {
        "id": "FuZZXids2YdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pser = pd.Series(np.random.randn(1000),\n",
        "  index = pd.date_range('1/1/2000', periods=1000))\n",
        "pdf = pd.DataFrame(np.random.randn(1000, 4), index=pser.index,\n",
        " columns=['A', 'B', 'C', 'D'])\n",
        "psdf = ps.from_pandas(pdf).head(10)\n",
        "print(psdf)\n",
        "print(\"##########\")\n",
        "psdf.to_csv('foo.csv')\n",
        "print(ps.read_csv('foo.csv'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D77uWlKyVLL",
        "outputId": "2753e956-a03e-4a70-fe4b-b2f78a956ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "/content/spark-3.4.0-bin-hadoop3/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   A         B         C         D\n",
            "2000-01-01 -1.595447 -0.791707  0.614005  0.098977\n",
            "2000-01-02 -0.527095  0.076967  0.399083 -1.250186\n",
            "2000-01-03  0.647316 -1.586751  0.956603 -1.979726\n",
            "2000-01-04 -0.752048  0.009426 -1.391053  0.351499\n",
            "2000-01-05 -0.002515  0.502028 -0.587403 -0.223349\n",
            "2000-01-06  0.937739  0.372121  0.620472  0.256358\n",
            "2000-01-07 -0.461002 -0.342698 -1.241647  0.668071\n",
            "2000-01-08 -1.071872  0.063163 -1.008257 -0.221815\n",
            "2000-01-09 -0.240724  1.380506 -1.902882  1.100550\n",
            "2000-01-10  0.901473 -0.705965  0.608965  0.790831\n",
            "##########\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.4.0-bin-hadoop3/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          A         B         C         D\n",
            "0 -1.595447 -0.791707  0.614005  0.098977\n",
            "1 -0.527095  0.076967  0.399083 -1.250186\n",
            "2  0.647316 -1.586751  0.956603 -1.979726\n",
            "3 -0.752048  0.009426 -1.391053  0.351499\n",
            "4 -0.002515  0.502028 -0.587403 -0.223349\n",
            "5  0.937739  0.372121  0.620472  0.256358\n",
            "6 -0.461002 -0.342698 -1.241647  0.668071\n",
            "7 -1.071872  0.063163 -1.008257 -0.221815\n",
            "8 -0.240724  1.380506 -1.902882  1.100550\n",
            "9  0.901473 -0.705965  0.608965  0.790831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запись и чтение Parquet."
      ],
      "metadata": {
        "id": "SgcC_gWeDZAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = ps.from_pandas(pdf).head(10)\n",
        "print(psdf)\n",
        "print(\"##########\")\n",
        "psdf.to_parquet('bar.parquet')\n",
        "ps.read_parquet('bar.parquet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "id": "h75rb67c7upC",
        "outputId": "5baffe48-c607-4aa0-a013-a8da09c8e637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "/content/spark-3.4.0-bin-hadoop3/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_parquet`, the existing index is lost when converting to Parquet.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
            "/content/spark-3.4.0-bin-hadoop3/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   A         B         C         D\n",
            "2000-01-01 -1.595447 -0.791707  0.614005  0.098977\n",
            "2000-01-02 -0.527095  0.076967  0.399083 -1.250186\n",
            "2000-01-03  0.647316 -1.586751  0.956603 -1.979726\n",
            "2000-01-04 -0.752048  0.009426 -1.391053  0.351499\n",
            "2000-01-05 -0.002515  0.502028 -0.587403 -0.223349\n",
            "2000-01-06  0.937739  0.372121  0.620472  0.256358\n",
            "2000-01-07 -0.461002 -0.342698 -1.241647  0.668071\n",
            "2000-01-08 -1.071872  0.063163 -1.008257 -0.221815\n",
            "2000-01-09 -0.240724  1.380506 -1.902882  1.100550\n",
            "2000-01-10  0.901473 -0.705965  0.608965  0.790831\n",
            "##########\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.4.0-bin-hadoop3/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_parquet`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          A         B         C         D\n",
              "0 -1.595447 -0.791707  0.614005  0.098977\n",
              "1 -0.527095  0.076967  0.399083 -1.250186\n",
              "2  0.647316 -1.586751  0.956603 -1.979726\n",
              "3 -0.752048  0.009426 -1.391053  0.351499\n",
              "4 -0.002515  0.502028 -0.587403 -0.223349\n",
              "5  0.937739  0.372121  0.620472  0.256358\n",
              "6 -0.461002 -0.342698 -1.241647  0.668071\n",
              "7 -1.071872  0.063163 -1.008257 -0.221815\n",
              "8 -0.240724  1.380506 -1.902882  1.100550\n",
              "9  0.901473 -0.705965  0.608965  0.790831"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.595447</td>\n",
              "      <td>-0.791707</td>\n",
              "      <td>0.614005</td>\n",
              "      <td>0.098977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.527095</td>\n",
              "      <td>0.076967</td>\n",
              "      <td>0.399083</td>\n",
              "      <td>-1.250186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.647316</td>\n",
              "      <td>-1.586751</td>\n",
              "      <td>0.956603</td>\n",
              "      <td>-1.979726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.752048</td>\n",
              "      <td>0.009426</td>\n",
              "      <td>-1.391053</td>\n",
              "      <td>0.351499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.002515</td>\n",
              "      <td>0.502028</td>\n",
              "      <td>-0.587403</td>\n",
              "      <td>-0.223349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.937739</td>\n",
              "      <td>0.372121</td>\n",
              "      <td>0.620472</td>\n",
              "      <td>0.256358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.461002</td>\n",
              "      <td>-0.342698</td>\n",
              "      <td>-1.241647</td>\n",
              "      <td>0.668071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1.071872</td>\n",
              "      <td>0.063163</td>\n",
              "      <td>-1.008257</td>\n",
              "      <td>-0.221815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.240724</td>\n",
              "      <td>1.380506</td>\n",
              "      <td>-1.902882</td>\n",
              "      <td>1.100550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.901473</td>\n",
              "      <td>-0.705965</td>\n",
              "      <td>0.608965</td>\n",
              "      <td>0.790831</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " API pandas в Spark полностью поддерживает различные источники данных Spark, такие как ORC и внешний источник данных. Запись и чтение ORC."
      ],
      "metadata": {
        "id": "CGoj_robD7mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psdf = ps.from_pandas(pdf).head(10)\n",
        "print(psdf)\n",
        "print(\"##########\")\n",
        "psdf.to_spark_io('zoo.orc', format=\"orc\")\n",
        "ps.read_spark_io('zoo.orc', format=\"orc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "ZOdFIWvfDyVv",
        "outputId": "a105a14c-8eea-4f56-a2cb-92ce38d2f5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
            "  series = series.astype(t, copy=False)\n",
            "/content/spark-3.4.0-bin-hadoop3/python/pyspark/pandas/frame.py:5364: FutureWarning: Deprecated in 3.2, Use DataFrame.spark.to_spark_io instead.\n",
            "  warnings.warn(\"Deprecated in 3.2, Use DataFrame.spark.to_spark_io instead.\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   A         B         C         D\n",
            "2000-01-01 -1.595447 -0.791707  0.614005  0.098977\n",
            "2000-01-02 -0.527095  0.076967  0.399083 -1.250186\n",
            "2000-01-03  0.647316 -1.586751  0.956603 -1.979726\n",
            "2000-01-04 -0.752048  0.009426 -1.391053  0.351499\n",
            "2000-01-05 -0.002515  0.502028 -0.587403 -0.223349\n",
            "2000-01-06  0.937739  0.372121  0.620472  0.256358\n",
            "2000-01-07 -0.461002 -0.342698 -1.241647  0.668071\n",
            "2000-01-08 -1.071872  0.063163 -1.008257 -0.221815\n",
            "2000-01-09 -0.240724  1.380506 -1.902882  1.100550\n",
            "2000-01-10  0.901473 -0.705965  0.608965  0.790831\n",
            "##########\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          A         B         C         D\n",
              "0 -1.595447 -0.791707  0.614005  0.098977\n",
              "1 -0.527095  0.076967  0.399083 -1.250186\n",
              "2  0.647316 -1.586751  0.956603 -1.979726\n",
              "3 -0.752048  0.009426 -1.391053  0.351499\n",
              "4 -0.002515  0.502028 -0.587403 -0.223349\n",
              "5  0.937739  0.372121  0.620472  0.256358\n",
              "6 -0.461002 -0.342698 -1.241647  0.668071\n",
              "7 -1.071872  0.063163 -1.008257 -0.221815\n",
              "8 -0.240724  1.380506 -1.902882  1.100550\n",
              "9  0.901473 -0.705965  0.608965  0.790831"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "      <th>D</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.595447</td>\n",
              "      <td>-0.791707</td>\n",
              "      <td>0.614005</td>\n",
              "      <td>0.098977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.527095</td>\n",
              "      <td>0.076967</td>\n",
              "      <td>0.399083</td>\n",
              "      <td>-1.250186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.647316</td>\n",
              "      <td>-1.586751</td>\n",
              "      <td>0.956603</td>\n",
              "      <td>-1.979726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.752048</td>\n",
              "      <td>0.009426</td>\n",
              "      <td>-1.391053</td>\n",
              "      <td>0.351499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.002515</td>\n",
              "      <td>0.502028</td>\n",
              "      <td>-0.587403</td>\n",
              "      <td>-0.223349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.937739</td>\n",
              "      <td>0.372121</td>\n",
              "      <td>0.620472</td>\n",
              "      <td>0.256358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.461002</td>\n",
              "      <td>-0.342698</td>\n",
              "      <td>-1.241647</td>\n",
              "      <td>0.668071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1.071872</td>\n",
              "      <td>0.063163</td>\n",
              "      <td>-1.008257</td>\n",
              "      <td>-0.221815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.240724</td>\n",
              "      <td>1.380506</td>\n",
              "      <td>-1.902882</td>\n",
              "      <td>1.100550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.901473</td>\n",
              "      <td>-0.705965</td>\n",
              "      <td>0.608965</td>\n",
              "      <td>0.790831</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вычисление корреляции между двумя рядами данных является обычной\n",
        "операцией в статистике. Мы spark.ml обеспечиваем гибкость расчета парных\n",
        "корреляций между многими рядами. Поддерживаемые методы корреляции в\n",
        "настоящее время — это корреляция Пирсона и Спирмена"
      ],
      "metadata": {
        "id": "A4k5txuiEhs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import Correlation\n",
        "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
        " (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
        " (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
        " (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
        "df = spark.createDataFrame(data, [\"features\"])\n",
        "r1 = Correlation.corr(df, \"features\").head()\n",
        "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
        "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
        "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyushELAEbcg",
        "outputId": "7843c361-d5d9-4be2-fe8c-14ffe0e66f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson correlation matrix:\n",
            "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
            "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
            "Spearman correlation matrix:\n",
            "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
            "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
            "             [       nan,        nan, 1.        ,        nan],\n",
            "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Еще один пример:"
      ],
      "metadata": {
        "id": "yKb1IFNXE5q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
        "from pyspark.ml.stat import Correlation\n",
        "dataset = [[Vectors.dense([1, 0, 0, -2])],\n",
        " [Vectors.dense([4, 5, 0, 3])],\n",
        " [Vectors.dense([6, 7, 0, 8])],\n",
        " [Vectors.dense([9, 0, 0, 1])]]\n",
        "dataset = spark.createDataFrame(dataset, ['features'])\n",
        "pearsonCorr = Correlation.corr(dataset, 'features', 'pearson').collect()[0][0]\n",
        "print(str(pearsonCorr).replace('nan', 'NaN'))\n",
        "spearmanCorr = Correlation.corr(dataset, 'features', method='spearman').collect()[0][0]\n",
        "print(str(spearmanCorr).replace('nan', 'NaN'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoCEdPIUE4y_",
        "outputId": "4035cdcc-26b8-4560-ab2f-abcacf3a5d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DenseMatrix([[1.        , 0.05564149,        NaN, 0.40047142],\n",
            "             [0.05564149, 1.        ,        NaN, 0.91359586],\n",
            "             [       NaN,        NaN, 1.        ,        NaN],\n",
            "             [0.40047142, 0.91359586,        NaN, 1.        ]])\n",
            "DenseMatrix([[1.        , 0.10540926,        NaN, 0.4       ],\n",
            "             [0.10540926, 1.        ,        NaN, 0.9486833 ],\n",
            "             [       NaN,        NaN, 1.        ,        NaN],\n",
            "             [0.4       , 0.9486833 ,        NaN, 1.        ]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChiSquareTest** проводит тест Пирсона на независимость каждой функции от\n",
        "этикетки. Для каждой функции пары (функция, метка) преобразуются в\n",
        "матрицу непредвиденных обстоятельств, для которой вычисляется статистика\n",
        "хи-квадрат. Все значения меток и функций должны быть категориальными.\n"
      ],
      "metadata": {
        "id": "3YReveY5FCUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.stat import ChiSquareTest\n",
        "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
        " (0.0, Vectors.dense(1.5, 20.0)),\n",
        " (1.0, Vectors.dense(1.5, 30.0)),\n",
        " (0.0, Vectors.dense(3.5, 30.0)),\n",
        " (0.0, Vectors.dense(3.5, 40.0)),\n",
        " (1.0, Vectors.dense(3.5, 40.0))]\n",
        "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
        "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
        "print(\"pValues: \" + str(r.pValues))\n",
        "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
        "print(\"statistics: \" + str(r.statistics))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp5DMfpLE9S5",
        "outputId": "4030925d-ee03-40c4-9614-92fdb0a363fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pValues: [0.6872892787909721,0.6822703303362126]\n",
            "degreesOfFreedom: [2, 3]\n",
            "statistics: [0.75,1.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Сумматор**\n",
        "\n",
        "Мы предоставляем сводную статистику векторного столбца для\n",
        "Dataframeсквозных Summarizer. Доступные метрики: максимальное,\n",
        "минимальное, среднее значение, сумма, дисперсия, стандартное значение и\n",
        "количество ненулевых значений по столбцам, а также общее количество."
      ],
      "metadata": {
        "id": "lDqLXa3eFRVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.stat import Summarizer\n",
        "from pyspark.sql import Row\n",
        "sc = spark.sparkContext\n",
        "df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
        " Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
        "# create summarizer for multiple metrics \"mean\" and \"count\"\n",
        "summarizer = Summarizer.metrics(\"mean\", \"count\")\n",
        "# compute statistics for multiple metrics with weight\n",
        "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
        "# compute statistics for multiple metrics without weight\n",
        "df.select(summarizer.summary(df.features)).show(truncate=False)\n",
        "# compute statistics for single metric \"mean\" with weight\n",
        "df.select(Summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
        "# compute statistics for single metric \"mean\" without weight\n",
        "df.select(Summarizer.mean(df.features)).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rgeJjhFFVuM",
        "outputId": "1e874c21-d2c1-4395-d6da-bc5848bb0575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+\n",
            "|aggregate_metrics(features, weight)|\n",
            "+-----------------------------------+\n",
            "|{[1.0,1.0,1.0], 1}                 |\n",
            "+-----------------------------------+\n",
            "\n",
            "+--------------------------------+\n",
            "|aggregate_metrics(features, 1.0)|\n",
            "+--------------------------------+\n",
            "|{[1.0,1.5,2.0], 2}              |\n",
            "+--------------------------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.0,1.0] |\n",
            "+--------------+\n",
            "\n",
            "+--------------+\n",
            "|mean(features)|\n",
            "+--------------+\n",
            "|[1.0,1.5,2.0] |\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
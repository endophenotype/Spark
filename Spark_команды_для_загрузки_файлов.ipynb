{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN++In/dv/HW/Q47jXX3oKe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/endophenotype/Spark/blob/main/Spark_%D0%BA%D0%BE%D0%BC%D0%B0%D0%BD%D0%B4%D1%8B_%D0%B4%D0%BB%D1%8F_%D0%B7%D0%B0%D0%B3%D1%80%D1%83%D0%B7%D0%BA%D0%B8_%D1%84%D0%B0%D0%B9%D0%BB%D0%BE%D0%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMXcmvz9czSG"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "ZBakr-9ue0re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fofQWtDafK9C",
        "outputId": "f716f86d-a0f8-4619-b583-ce547b596e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "9nSxSa8sfPnA",
        "outputId": "6d274e57-85c2-46c3-9efd-9f5d62414673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x780ed019bd90>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://561ce7dcf0e1:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        " .builder \\\n",
        " .appName(\"Python Spark SQL basic example\") \\\n",
        " .config(\"spark.some.config.option\", \"some-value\") \\\n",
        " .getOrCreate()\n",
        "df = spark.read.load(\"/content/spark-3.1.1-bin-hadoop3.2/examples/src/main/resources/users.parquet\")\n",
        "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSmBucDFfXtQ",
        "outputId": "7b33a39d-dd9b-4cf6-ae67-e578b1e8ca0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+----------------+\n",
            "|  name|favorite_color|favorite_numbers|\n",
            "+------+--------------+----------------+\n",
            "|Alyssa|          null|  [3, 9, 15, 20]|\n",
            "|   Ben|           red|              []|\n",
            "+------+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно использовать для загрузки JSON файлов"
      ],
      "metadata": {
        "id": "AVvIRIrFjx99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession \\\n",
        " .builder \\\n",
        " .appName(\"Python Spark SQL basic example\") \\\n",
        " .config(\"spark.some.config.option\", \"some-value\") \\\n",
        " .getOrCreate()\n",
        "df = spark.read.load(\"/content/spark-3.1.1-bin-hadoop3.2/examples/src/main/resources/people.json\",\n",
        "format=\"json\")\n",
        "df.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "ZS-gxfUfjX1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676a04ef-cbcf-4f43-8c05-49b9394feeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|null|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно использовать для загрузки CSV файлов"
      ],
      "metadata": {
        "id": "LUBX4Q1skinV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        " .builder \\\n",
        " .appName(\"Python Spark SQL basic example\") \\\n",
        " .config(\"spark.some.config.option\", \"some-value\") \\\n",
        " .getOrCreate()\n",
        "df = spark.read.load(\"/content/spark-3.1.1-bin-hadoop3.2/examples/src/main/resources/people.csv\", \\\n",
        " format=\"csv\", sep=\";\", inferSchema=\"true\", header=\"true\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "agp8_Gv6jmAP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c8e6158-016d-4eb1-97ba-31b4d3b9b9e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+---------+\n",
            "| name|age|      job|\n",
            "+-----+---+---------+\n",
            "|Jorge| 30|Developer|\n",
            "|  Bob| 32|Developer|\n",
            "+-----+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Источник данных ORC:"
      ],
      "metadata": {
        "id": "BkAKPDSTmLwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        " .builder \\\n",
        " .appName(\"Python Spark SQL basic example\") \\\n",
        " .config(\"spark.some.config.option\", \"some-value\") \\\n",
        " .getOrCreate()\n",
        "df = spark.read.orc(\"/content/spark-3.1.1-bin-hadoop3.2/examples/src/main/resources/users.orc\")\n",
        "(df.write.format(\"orc\")\n",
        " .option(\"orc.bloom.filter.columns\", \"favorite_color\")\n",
        " .option(\"orc.dictionary.key.threshold\", \"1.0\")\n",
        " .option(\"orc.column.encoding.direct\", \"name\")\n",
        " .save(\"users_with_options.orc\"))"
      ],
      "metadata": {
        "id": "5_4ACwqcmEpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYru8X7Zq5HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Источник данных о parquet:"
      ],
      "metadata": {
        "id": "iCUyD0W5mjww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        " .builder \\\n",
        " .appName(\"Python Spark SQL basic example\") \\\n",
        " .config(\"spark.some.config.option\", \"some-value\") \\\n",
        " .getOrCreate()\n",
        "df = spark.read.parquet(\"/content/spark-3.1.1-bin-hadoop3.2/examples/src/main/resources/users.parquet\")\n",
        "(df.write.format(\"parquet\")\n",
        " .option(\"parquet.bloom.filter.enabled#favorite_color\", \"true\")\n",
        " .option(\"parquet.bloom.filter.expected.ndv#favorite_color\", \"1000000\")\n",
        " .option(\"parquet.enable.dictionary\", \"true\")\n",
        " .option(\"parquet.page.write-checksum.enabled\", \"false\")\n",
        " .save(\"users_with_options.parquet\"))\n"
      ],
      "metadata": {
        "id": "K1LRp0_AmWfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запуск SQL для файлов напрямую"
      ],
      "metadata": {
        "id": "nzuL-2gGmtIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        " .builder \\\n",
        " .appName(\"Python Spark SQL basic example\") \\\n",
        " .config(\"spark.some.config.option\", \"some-value\") \\\n",
        " .getOrCreate()\n",
        "df = spark.sql(\"SELECT * FROM parquet.`/content/spark-3.1.1-bin-hadoop3.2/examples/src/main/resources/users.parquet`\")\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7Wqzg0QmtyW",
        "outputId": "c3e7f3d4-719b-4857-cc84-375f7f51e5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+----------------+\n",
            "|  name|favorite_color|favorite_numbers|\n",
            "+------+--------------+----------------+\n",
            "|Alyssa|          null|  [3, 9, 15, 20]|\n",
            "|   Ben|           red|              []|\n",
            "+------+--------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        " .builder \\\n",
        " .appName(\"Python Spark SQL basic example\") \\\n",
        " .config(\"spark.some.config.option\", \"some-value\") \\\n",
        " .getOrCreate()\n",
        "df = spark.read.parquet(\"/content/spark-3.1.1-bin-hadoop3.2/examples/src/main/resources/users.parquet\")\n",
        "# $example on:write_sorting_and_bucketing$\n",
        "df.write.bucketBy(42, \"name\").sortBy(\"name\").saveAsTable(\"people_bucketed\")"
      ],
      "metadata": {
        "id": "0s7bFIdznuGF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
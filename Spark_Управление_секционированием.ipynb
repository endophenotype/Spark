{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxoIOU3PoMqv5M3VMX2DBx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/endophenotype/Spark/blob/main/Spark_%D0%A3%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81%D0%B5%D0%BA%D1%86%D0%B8%D0%BE%D0%BD%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%D0%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMXcmvz9czSG"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "ZBakr-9ue0re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fofQWtDafK9C",
        "outputId": "fa5a975f-2c07-4a81-da41-2d5e407e53fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "9nSxSa8sfPnA",
        "outputId": "f76d83bd-e29d-43d7-d714-9bff1c1774a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f5d101a9240>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a32cf319488a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Следующий код использует reduceByKey операцию над парами\n",
        "ключ-значение, чтобы подсчитать, сколько раз каждая строка текста\n",
        "встречается в файле"
      ],
      "metadata": {
        "id": "1KL14YfpjUxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "appName = 'appName'\n",
        "master = 'local[1]'\n",
        "conf = SparkConf().setAppName(appName).setMaster(master)\n",
        "sc = spark.sparkContext\n",
        "lines = sc.textFile(\"/content/P.txt\")\n",
        "pairs = lines.map(lambda s: (s, 1))\n",
        "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
        "for element in counts.collect():\n",
        " print(element)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9At_xQGhi3dk",
        "outputId": "05df2dde-f856-46d6-faca-9595fe736378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Python', 4)\n",
            "('python', 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Широковещательные переменные создаются из переменной v путем вызова\n",
        "SparkContext.broadcast(v). Широковещательная переменная представляет\n",
        "собой оболочку вокруг v, и ее значение можно получить, вызвав value метод.\n",
        "Код ниже показывает это:"
      ],
      "metadata": {
        "id": "bzR5NkPzj8kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "master = 'local[*]'\n",
        "sc.stop()\n",
        "conf = SparkConf().setAppName(appName).setMaster(master)\n",
        "sc = SparkContext(conf=conf)\n",
        "broadcastVar = sc.broadcast([1, 2, 3])\n",
        "print(broadcastVar)\n",
        "broadcastVar.value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoFYhL-BjaiW",
        "outputId": "1ad2938e-8c25-402c-bf4f-5b5b46145568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.broadcast.Broadcast object at 0x7f5d101adf30>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В приведенном ниже коде показан аккумулятор, используемый для сложения\n",
        "элементов массива:"
      ],
      "metadata": {
        "id": "3w55xQHRkT2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()\n",
        "conf = SparkConf().setAppName(appName).setMaster(master)\n",
        "sc = SparkContext(conf=conf)\n",
        "accum = sc.accumulator(0)\n",
        "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))\n",
        "accum.value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LClA9MNukD4w",
        "outputId": "7d261822-881b-4f1e-a172-6c040f9df97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Интерфейс AccumulatorParam имеет два метода: zero для предоставления\n",
        "«нулевого значения» для вашего типа данных и addInPlace для сложения двух\n",
        "значений вместе. Например, предположим, что у нас есть Vector класс,\n",
        "представляющий математические векторы, мы могли бы написать:"
      ],
      "metadata": {
        "id": "4zwXu0gCkduG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.accumulators import AccumulatorParam\n",
        "class VectorAccumulatorParam(AccumulatorParam):\n",
        "  def zero(self, initialValue):\n",
        "    return [0.0] * len(initialValue)\n",
        "  def addInPlace(self, v1, v2):\n",
        "    v1 += v2\n",
        "    return v1\n",
        "vecAccum = sc.accumulator([1, 2, 3], VectorAccumulatorParam())\n",
        "vecAccum.value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTY_Zvdgk35r",
        "outputId": "7e5c34c2-f973-4918-e26d-b5f6cc4d721c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accum = sc.accumulator(0)\n",
        "def g(x):\n",
        "  global vecAccum\n",
        "  accum.add(x)\n",
        "  return f(x)\n",
        "rdd = sc.parallelize(vecAccum.value)\n",
        "rdd.map(g)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1GRwDUqmT9s",
        "outputId": "2bbfff35-b2f5-4e8d-ca3d-4ad5071d1214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[9] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "appName = 'appName'\n",
        "master = 'local[*]'\n",
        "sc.stop()\n",
        "conf = SparkConf().setAppName(appName).setMaster(master)\n",
        "sc = SparkContext(conf=conf)\n",
        "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
        "print(nums.collect())\n",
        "bytwo = nums.map(lambda x: x + 2)\n",
        "print(bytwo.collect())\n",
        "squared = nums.map(lambda x: x * x)\n",
        "print(squared.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-Y5qbNnzZuR",
        "outputId": "ec14e426-54ad-4330-df62-33c7fbe4b6ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n",
            "[3, 4, 5, 6, 7]\n",
            "[1, 4, 9, 16, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark имеет как стандартный, так и пользовательский разделитель. Это означает, что когда вы создаете RDD, вы можете позволить Spark установить количество разделов или указать его явно. Количество разделов по умолчанию зависит от источника данных, размера кластера и доступных ресурсов. В большинстве случаев разделение по умолчанию будет работать нормально, но если вы опытный программист Spark, вы можете предпочесть явно указать количество разделов с помощью RDD.repartition, RDD.coalesce() или DataFrame.coalesce(). функция.\n",
        "Spark предлагает несколько функций для управления секционированием. Вы можете использовать RDD.repartition(numPartitions), чтобы вернуть новый RDD, который имеет ровно numPartitions разделов. Эта функция может увеличивать или уменьшать уровень параллелизма в RDD, как показано в следующем примере:\n"
      ],
      "metadata": {
        "id": "L5oCcAzc1EO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "appName = 'appName'\n",
        "master = 'local[*]'\n",
        "sc.stop()\n",
        "conf = SparkConf().setAppName(appName).setMaster(master)\n",
        "sc = SparkContext(conf=conf)\n",
        "rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10], 5)\n",
        "print('rdd.getNumPartitions() ',rdd.getNumPartitions())\n",
        "print('sorted(rdd.glom().collect()) ', sorted(rdd.glom().collect()))\n",
        "print('len(rdd.repartition(4).glom().collect()) ', len(rdd.repartition(4).glom().collect()))\n",
        "rdd = rdd.repartition(4).glom()\n",
        "print('rdd.getNumPartitions() ',rdd.getNumPartitions())\n",
        "print('sorted(rdd.glom().collect()) ', sorted(rdd.glom().collect()))\n",
        "print('len(rdd.repartition(2).glom().collect()) ', len(rdd.repartition(2).glom().collect()))\n",
        "rdd = rdd.repartition(2).glom()\n",
        "print('rdd.getNumPartitions() ',rdd.getNumPartitions())\n",
        "print('sorted(rdd.glom().collect()) ', sorted(rdd.glom().collect()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmhDqR_j1D57",
        "outputId": "5a35745d-195e-4984-9bc5-848d5e8adc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rdd.getNumPartitions()  5\n",
            "sorted(rdd.glom().collect())  [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]\n",
            "len(rdd.repartition(4).glom().collect())  4\n",
            "rdd.getNumPartitions()  4\n",
            "sorted(rdd.glom().collect())  [[[1, 2]], [[3, 4]], [[5, 6, 7, 8]], [[9, 10]]]\n",
            "len(rdd.repartition(2).glom().collect())  2\n",
            "rdd.getNumPartitions()  2\n",
            "sorted(rdd.glom().collect())  [[[[3, 4]]], [[[5, 6, 7, 8], [9, 10], [1, 2]]]]\n"
          ]
        }
      ]
    }
  ]
}